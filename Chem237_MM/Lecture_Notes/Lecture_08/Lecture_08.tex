\documentclass{article}
%==============================================================================%
%	                          Packages                                     %
%==============================================================================%
% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\usepackage[margin=0.7in]{geometry}
\usepackage[version=4]{mhchem}
%==============================================================================%
%                           User-Defined Commands                              %
%==============================================================================%
% User-Defined Commands
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\pd}{\partial}
\newcommand{\dg}{\dagger}
\newcommand{\sumzero}{\sum_{n=0}^\infty}
\newcommand{\sumone}{\sum_{n=1}^\infty}
%==============================================================================%
%                             Title Information                                %
%==============================================================================%
\title{Chem237: Lecture 8}
\date{4/10/18}
\author{Alan Robledo}
%==============================================================================%
%	Everyone Please Make Comments if Something Needs to be Reviewed        %
%                           Or just fix it yourself!                           %
%==============================================================================%
\begin{document}
\maketitle

\subsection*{Approximate Methods}
So far, we have discussed different ways to evaluate integrals of interest to obtain exact answers.
It is often times enough for us to know an approximation to the value of an integral and we will discuss a few methods that go about this by looking at some special functions as examples.
\subsubsection*{Asymptotic Series}
At first glance, you'd think that the error function is just the integral of a gaussian
\be
  \text{erf(x)} = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt = \frac{1}{\sqrt{\pi}} \int_{-x}^x e^{-t^2} dt
\ee
and you'd be correct. To be specific, the error function is the integral of a normalized Gaussian with the normalization part coming from the $\frac{1}{\sqrt{\pi}}$.
The error function comes up in probability theory as this function is used to show the probability of something lying between -x and x.
Finding a good approximation of the integral can be done by taylor expanding the integrand about a center that we will take to be zero, and integrating term by term.
\be
  \begin{split}
    \text{erf(x)} &= \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt \\
    &= \frac{2}{\sqrt{\pi}} \int_0^x (1 - t^2 + \frac{t^4}{2!} - \frac{t^6}{3!} + \hdots) dt \\
    &= \frac{2}{\sqrt{\pi}} (x - \frac{x^3}{3} + \frac{x^5}{5 \cdot 2!} - \frac{x^7}{7 \cdot 3!} + \hdots)
  \end{split}
\ee
By the look of it, we can say that the series converges for any x because the denominator tends to infinity much faster than the numerator ever could.
However, the convergence of the series is small for large x.
As a matter of fact, for large x, as x $\rightarrow \infty$, erf(x) $\rightarrow$ 1.
So suppose we wanted to know what erf(x) is for large x.
Using the fact that erf(x) $\rightarrow$ 1 as x $\rightarrow \infty$, we could write
\be
1 = \frac{2}{\sqrt{\pi}} \int_0^{\infty} e^{-t^2} dt
\ee
and if we break up the integrals
\be
1 = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt + \frac{2}{\sqrt{\pi}} \int_x^{\infty} e^{-t^2} dt
\ee
and rearrange terms, we get
\be \label{eq:erfc}
1 - \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt = 1 - \text{erf(x)} = \frac{2}{\sqrt{\pi}} \int_x^{\infty} e^{-t^2} dt = \text{erfc(x)}
\ee
where we have an integral from large x to infinity, which happens to be called the complementary error function or erfc(x).
We can try to find an expression for the complementary error function by considering using integration by parts.
Now, you may look at the integrand and think to yourself that there are no two functions that you can use for integration by parts.
But a neat little trick that is often used is to consider multiplying the integrand by the one function.
\be
  \int_x^{\infty} e^{-t^2} dt = \int_x^{\infty} e^{-t^2} \cdot 1 dt = \int_x^{\infty} e^{-t^2} \cdot \frac{-2t}{-2t} dt
\ee
and if we consider,
\be
  \begin{split}
    U &= - \frac{1}{2t} \qquad dV = - 2t e^{-t^2} dt \\
    dU &= \frac{1}{2t^2} dt \qquad V = e^{-t^2}
  \end{split}
\ee
our integral becomes
\be
  \begin{split}
    \int_x^{\infty} e^{-t^2} dt &= \frac{e^{-t^2}}{2t} \Big|_x^{\infty} - \int_x^{\infty} \frac{e^{-t^2}}{2t^2} dt \\
    &= \frac{e^{-x^2}}{2x} - \int_x^{\infty} \frac{e^{-t^2}}{2t^2} dt
  \end{split}
\ee
We can integrate by parts again by using the same trick of multiplying the integrand by the one function
\be
  \int_x^{\infty} e^{-t^2} dt = \frac{e^{-x^2}}{2x} - \int_x^{\infty} \frac{e^{-t^2}}{2t^2} \cdot \frac{-2t}{-2t} dt
\ee
and considering
\be
  \begin{split}
    U &= - \frac{1}{4t^3} \qquad dV = - 2t e^{-t^2} dt \\
    dU &= \frac{3}{4t^4} dt \qquad V = e^{-t^2}
  \end{split}
\ee
to get
\be
  \begin{split}
    \int_x^{\infty} e^{-t^2} dt &= \frac{e^{-x^2}}{2x} - \int_x^{\infty} \frac{e^{-t^2}}{2t^2} \\
    &= \frac{e^{-x^2}}{2x} + \frac{e^{-t^2}}{4t^3} \Big|_x^{\infty} + \int_x^{\infty} \frac{3e^{-t^2}}{4t^4} dt \\
    &= \frac{e^{-x^2}}{2x} - \frac{e^{-x^2}}{4x^3} + \int_x^{\infty} \frac{3e^{-t^2}}{4t^4} dt
  \end{split}
\ee
You can see that we can use this trick each time we integrate by parts and if we integrated an infinite number of times, we'd have an exact expression.
If we go back to the equation \ref{eq:erfc} and solve for erf(x), we can see that the result after n integrations by parts of the complementary error function gives
\be
  \begin{split}
    \text{erf(x)} = 1 - \frac{2}{\sqrt{\pi}} \int_x^{\infty} e^{-t^2} dt = 1 - \frac{2}{\sqrt{\pi}} e^{-x^2} \Big[ \frac{1}{2x} - \frac{1}{2^2 x^3} & + \frac{1 \cdot 3}{2^3 x^5} - \frac{1 \cdot 3 \cdot 5}{2^4 x^7} + \hdots + (-1)^{n-1} \frac{1 \cdot 3 \cdot 5 \cdots (2n-3)}{2^n x^{2n-1}} \Big] \\
    & + (-1)^n \frac{1 \cdot 3 \cdot 5 \cdots (2n-1)}{2^n} \frac{2}{\sqrt{\pi}} \int_x^{\infty} \frac{e^{-t^2}}{t^{2n}} dt
  \end{split}
\ee
where the last term (the term after the brackets, including the integral) is the "remainder" that makes the expression exact.
If we only pay attention to the terms inside the brackets, we can see that these terms form the beginning of divergent series.
Since the numerator grows as (2n+1)!, there is no value of x that would make the denominator grow larger in size than the numerator so if we were to form an infinite series, the series would diverge to $\infty$.
It is because of this fact that physicists would call the series in brackets an \textbf{asymptotic series} if the summation of the terms continued to infinity.
To be formal, if we have a function whose series expansion is
\be
S(x) = c_0 + \frac{c_1}{x} + \frac{c_2}{x} + \hdots
\ee
where the c$_n$'s are just constants, we can say that the partial sum $\sum\limits_{i=0}^n c_i / x^i$ is an asymptotic series expansion of f(x) if the following holds
\be
  \lim_{x \to \infty} x^n \Big[ f(x) - \sum_{i=0}^n \frac{c_i}{x^i} \Big] = 0
\ee
Therefore, for some value n and for arbitrarily large x, the series $S_n(x)$ gives a good approximation to f(x).
If, however, we have a function whose series expansion is
\be
  S(x) = c_0 + c_1x + c_2x^2 + \hdots
\ee
where the c$_n$'s are again just constants, we can say that the partial sum  $\sum\limits_{i=0}^n c_i x^i$ is an asymptotic series expansion of f(x) if the following holds
\be
  \lim_{x \to \infty} \frac{1}{x^n} \Big[ f(x) - \sum_{i=0}^n c_i x^i \Big] = 0
\ee
Therefore, for some value n and for arbitrarily small x, the series $S_n(x)$ gives a good approximation to f(x).

Mathematicians define an asymptotic series expansion as a series whose partial sums give a good approximation to a function of a variable x for arbitrarily small or large values of x.
This definition makes it sound like a convergent Taylor series falls under the category of an asymptotic expansion.
Because of this, it is common in Physics for the term 'asymptotic expansion' to imply a divergent series.
To be mathematically correct, an asymptotic series can be divergent or convergent but, in most cases of interest, asymptotic series never converge.
So it is useful to consider the physicist's perspective when distinguishing between a convergent series and an asympotitc series.
We can go back to the error function erf(x) to show how an asymtptic series can be different from a convergent series.
\be
  \text{erf(x)} = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt
\ee
Just as we did in equation 2, if we taylor expand the integrand about $t=0$, and integrate the series term by term, we get the following convergent series,
\be
  \text{erf(x)} = \frac{2}{\sqrt{\pi}} \Big( x - \frac{1}{3}x^3 + \hdots + \frac{(-1)^n}{(2n+1)n!} x^{2n+1} + \hdots \Big)
\ee
which is convergent for all values of x.
And if we recall from earlier, we obtained an asymptotic series of the erf(x)
\be
  \text{erf(x)} \approx 1 - \frac{2}{\sqrt{\pi}} e^{-x^2} \Big[ \frac{1}{2x} - \frac{1}{2^2 x^3} + \frac{1 \cdot 3}{2^3 x^5} - \frac{1 \cdot 3 \cdot 5}{2^4 x^7} + \hdots + (-1)^{n-1} \frac{1 \cdot 3 \cdot 5 \cdots (2n-3)}{2^n x^{2n-1}} \Big] + \text{remainder}
\ee
Now, when $x=3$, we need the first 30 terms of the taylor series to approximate erf(3) to an accuracy of $10^{-5}$, whereas we only need about the first two terms of the asymptotic series (excluding the remainder of course) to obtain the same approximation.
This should not be of any surprise to us because I mentioned earlier in the lecture that the convergence of the Taylor series is slow for (arbitrarily) large x.
If we were to make a plot of the remainder as a function of the number of terms we decide to keep in our series approximation for fixed x
%==============================================================================%
%  Add plot of remainder as a function of M in notes.
%==============================================================================%
we would see that as we keep adding more terms, our approximation gets better and better up to a certain limit, which defines the asymptoticity of the function, and then our approximation becomes worse and worse.
This should make sense because we are trying to use a divergent series to define the value of an integral.

Note, if we use the physics definition of asymptotic series then we have to be clear in saying that not all functions have an asymptotic expansion.
If a function does have such an expansion then that expansion is unique.
However, one can get different asympotitc series expansions for the same function when we consider x to be a complex number z and the convergence of a series to be defined by the radius of a disk on the complex plane.
This is known in complex analysis as Stokes phenomenon but we will not dive into the topic.

\subsubsection*{Saddle Point Method}
Consider the integral
\be
  \int_0^{\infty} F(x) dx = \int_0^{\infty} e^{f(x)} dx
\ee
where the integral of F(x) is dominated by a narrow region in x.
A example plot of F(x) as a function of x would be
%==============================================================================%
%  Add plot of remainder as a function of M in notes.
%==============================================================================%
If we perform a Taylor expansion of f(x) and decide to expand around the point x$_o$ we get
\be
  F(x) \approx \text{exp} \Big[ f(x_o) + (x - x_o)f'(x_o) + \frac{(x - x_o)^2}{2}f''(x_o) \Big]
\ee
If we let x$_o$ to be the maximum of f(x), we can identify that $f'(x_o) = 0$ so our integral becomes
\be
  \begin{split}
    \int_0^{\infty} F(x) dx &\approx \int_0^{\infty} \text{exp} \big[ f(x_o) + \frac{(x - x_o)^2}{2}f''(x_o) \Big] dx \\
    &= \text{exp}[f(x_o)] \int_0^{\infty} \text{exp} \Big[ \frac{(x - x_o)^2}{2}f''(x_o) \Big] dx
  \end{split}
\ee
which is just the integral of a gaussian exp $[A(x-x_o)^2]$ where $A = \frac{f''(x_o)}{2}$.
Using a u-substitution $u = x - x_o$ and $du = dx$, you should be able to get
\be
  \begin{split}
      \int_0^{\infty} F(x) dx &\approx \text{exp}[f(x_o)] \int_0^{\infty} \text{exp} \Big[ \frac{(x - x_o)^2}{2}f''(x_o) \Big] dx \\
      &= \text{exp}[f(x_o)] \Big( \frac{-2 \pi}{f''(x_o)} \Big) ^{\frac{1}{2}}
  \end{split}
\ee
where $f''(x_o) < 0$ to keep from getting imaginary results.
As an example, we will consider another special function: the Gamma function.
\be
  \Gamma(x+1) = \int_0^{\infty} t^{x} e^{-t} dt
\ee
We can turn the integrand into an exponential with the following transformation
\be
  t^{x} e^{-t} = e^{\ln(t^x)} e^{-t} = e^{x \ln(t)} e^{-t} = e^{x \ln(t) - t}
\ee
and use the saddle-point approximation to approximate the value of the integral.
We will consider $f(t) = x \ln(t) - t$ and Taylor expand f(t) up to second order.
So we need to compute some derivatives and evaluate them at the maximum $t_o$.
\be
  \begin{split}
    f(t) \Big|_{t=t_o} &= x \ln(t_o) - t_o \\
    f'(t) \Big|_{t=t_o} &= \frac{x}{t_o} - 1 \\
    f''(t) \Big|_{t=t_o} &= -\frac{x}{t_o^2}
  \end{split}
\ee
Since we know that the value of the first derivative at the maximum is zero, we can use the second equation to solve for zero and we get the relationship $t_o = x$.
Replacing all the $t_o$'s with x, we get the following expansion
\be
  f(t) \approx x \ln(x) - x - \frac{1}{2!} \cdot \frac{1}{x} (t - x)^2
\ee
So our integral becomes
\be
  \begin{split}
    \Gamma(x+1) &= \int_0^{\infty} t^{x} e^{-t} dt \\
    &= \int_0^{\infty} e^{x \ln(x) - x - \frac{1}{2x} (t - x)^2} \\
    &= \int_0^{\infty} e^{x \ln(x) - x} e^{- \frac{1}{2x} (t - x)^2} dt \\
    &= e^{\ln(x^x) - x} \int_0^{\infty} e^{- \frac{1}{2x} (t - x)^2} dt \\
    &= x^x e^{-x} \int_0^{\infty} e^{- \frac{1}{2x} (t - x)^2} dt
  \end{split}
\ee
which is of course another integral of a gaussian.
Using a u-substitution $u = t - x$ and $du = dx$ (remember, x is considered a constant in this integral) we have
\be
  \begin{split}
    \Gamma(x+1) &\approx x^x e^{-x} \int_0^{\infty} e^{- \frac{1}{2x} (t - x)^2} dt \\
    &= x^x e^{-x} \int_0^{\infty} e^{- \frac{1}{2x} u^2} du \\
    &= \sqrt{2 \pi x} x^x e^{-x} \\
    &= \sqrt{2 \pi x} \Big( \frac{x}{e} \Big)^x
  \end{split}
\ee
which is actually the first half of proving stirling's approximation!
The second half of the proof involves showing that $\Gamma(x+1) = x!$, which can be done by using multiple integration by parts, which shouldn't be too bad.
Taking $t^x = U$ and $e^{-t} = dV$, we get
\be
  \begin{split}
    \Gamma(x+1) &= \int_0^{\infty} t^{x} e^{-t} dt \\
    &= - t^x e^{-t} \Big|_0^{\infty} + x \int_0^{\infty} t^{x-1} e^{-t} dt \\
  \end{split}
\ee
and since $- t^x e^{-t} \Big|_0^{\infty}$ goes to zero, we're just left with $\Gamma(x)$.
So we can integrate by parts a few more times until we see a pattern
\be
  \begin{split}
    \Gamma(x+1) &= x \Gamma(x) \\
    &= x \int_0^{\infty} t^{x-1} e^{-t} dt \\
    &= x \Big( -t^{x-1}e^{-t} \Big|_0^{\infty} + (x-1) \int_0^{\infty} t^{x-2} e^{-t} dt \Big) \\
    &= x(x-1) \int_0^{\infty} t^{x-2} e^{-t} dt \\
    &= x(x-1) \Gamma(x) \\
    &= \hdots \\
    &= x(x-1)\cdot2\cdot1 \int_0^{\infty} e^{-t} dt \\
    &= - (x!) e^{-t} \Big|_0^{\infty} \\
    &= -x! (0 - 1)\\
    &= x!
  \end{split}
\ee
And from our result from earlier, we have that for any integer x = n
\be
  \Gamma(n+1) = n! \approx \sqrt{2 \pi n} \Big( \frac{n}{e} \Big)^n
\ee
which is Stirling's approximation!
%==============================================================================%
% Relate it to Stat mech if you want.
%==============================================================================%
And with that, we are done with chapter 3 of the Math Methods book.
\subsection*{Integral Transforms}
Chapter 4 can be thought of as a continuation of chapter 3 in that we are still going to be looking at integrals and finding ways to solve them.
But, similar to contour integration, we are going to be focusing on how to map our function of interest from one domain onto another domain in order to make our integrals much easier to solve.
Then we would map our solutions back to the original domain to get the appropriate result.
This is done by considering different types of integral transforms that are used depending on the behavior of our function/integrand.
Some include: Fourier Transform, Laplace Transform, Hilbert Transform, etc, but we will only be discussing the first two as they are very commonly used in Physics.
\subsubsection*{Fourier Series}
The topic of Fourier Series/Transform is one that is discussed often in physics classes and some Physical Chemistry classes.
However, the topic is usually introduced through example and you are only required to know how to obtain the series and/or perform the transform and then understand how it can be applied.
Our Math Methods book introduces the topic the same way.
For those of you who have been taught the subject this way or for those who have only heard of it, you might be aware that there are different conventions associated with the Series/Transform.
But, despite the existence of different conventions, each give the same results to the same problems.

The concept of a Fourier Series is made more intuitive when understood in the context of \textbf{Linear Vector Spaces}.
Linear Vector Spaces are discussed heavily in Linear Algebra courses and are thought about constantly in Quantum Mechanics.
We will be covering Linear Algebra later in the course, but if you have not been exposed to Linear Algebra you can refer to Lecture 12 where we discuss it in full detail.
Or you can try following along until you don't understand something and keep going back and forth with both lectures and whatever other material you may be using.
We will start with basic general definitions that are discussed in any textbook but we will soon make the connection to vector spaces to get a better understanding of where things come from.

When we have a function $f(\theta)$ that is defined periodically for $0 \leq \theta \leq 2 \pi$, we can create an expansion that is of the form
\be
  f(\theta) = \sum_{n = 0}^{\infty} A_n \cos(n \theta) + B_n \sin(n \theta)
\ee
This is considered the general form of a fourier series and the coefficients can be found analytically.
When we say that a function is periodic, we mean that the function repeats in intervals $- 2\pi \leq \theta \leq 0$, $- \pi \leq \theta \leq \pi$, $2 \pi \leq \theta \leq 4 \pi $, etc.
It is only required that the interval is of length $2\pi$.
If we extend or contract the length of the interval, we must change the form of the sine and cosine accordingly.
For the time being, we will stick to the interval $0 \leq \theta \leq 2 \pi$ and discuss changing the length of the interval later.

One of the nice properties of sine and cosine functions is that they are orthogonal on given intervals.
What it means for our trig functions to be orthogonal is that if we have two functions $\sin(nx)$ and $\cos(mx)$ and integrate their product over the interval $0 \leq x \leq 2 \pi$ we get
\be \label{eq:sin_cos}
  \int_0^{2\pi} \sin(nx)\cos(mx) dx = 0
\ee
for any n and m.
And if take the the product of two sine functions $\sin(nx)$ and $\sin(mx)$, then
\be
  \begin{split}
    \int_0^{2\pi} \sin(nx)\sin(mx) dx &= 0 \quad \text{if} \quad n \neq m \\
    \int_0^{2\pi} \sin(nx)\sin(mx) dx &= \pi \quad \text{if} \quad n = m
  \end{split}
\ee
Likewise, if we have two cosine functions $\cos(nx)$ and $\cos(mx)$, then
\be
  \begin{split}
    \int_0^{2\pi} \cos(nx)\cos(mx) dx &= 0 \quad \text{if} \quad n \neq m \\
    \int_0^{2\pi} \cos(nx)\cos(mx) dx &= \pi \quad \text{if} \quad n = m
  \end{split}
\ee
where n and m are integers $n,m = 1, 2, 3, \hdots$.
If we wanted to normalize our trig functions, then we must consider every sine and cosine having a normalization constant $a_n$ and $b_n$ such that the product of sines is equal to one
\be \label{eq:sin_sin}
  \int_0^{2\pi} a_n^2 \sin^2(nx) = a_n^2\pi = 1 \Rightarrow a_n = \frac{1}{\sqrt{\pi}}
\ee
and the product of cosines is also equal to one
\be \label{eq:cos_cos}
  \int_0^{2\pi} a_n^2 \cos^2(nx) = b_n^2\pi = 1 \Rightarrow b_n = \frac{1}{\sqrt{\pi}}
\ee
Whether your decide to normalize your sines and cosines is one convention while not normalizing your sines and cosines is another convention.
We will use the convention of using normalized sines and cosines so that these integrals either evaluate to 0 or 1 for simplicity.

To introduce the concept of a linear vector space, remember that whenever we have a function, we can almost always represent it in a Taylor series expansion, giving us
\be
  f(x) = \sum_{n=0}^{\infty} h_n x^n
\ee
and (assuming our function can be Taylor expanded) we can think of the function as having a basis of polynomials of the form $x^n$ and weightings $h_n$ that are of the form
\be
  h_n = \frac{f^{(n)}(x_o)}{n!}
\ee
From lecture 3, we showed that sine and cosine both have Taylor expansions of the form
\be
  \begin{split}
    \sin(x) &= 0 + x + 0\frac{x^2}{2!} - \frac{x^3}{3!} + \hdots = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \hdots \\
    \cos(x) &= 1 + 0x - \frac{x^2}{2!} + 0 \frac{x^3}{3!} + \hdots = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \hdots
  \end{split}
\ee
which means that we could also think of our fourier series as having a basis of polynomials of the form $x^n$.
Because we defined our Fourier series and our Taylor series as a sum from zero to infinity, this means that we have an infinite number of polynomials.
So if we consider a set of all of these polynomials together with the addition of polynomials and the multiplication of a polynomial by a number, we have an infinite dimensional linear vector space where our polynomials themselves are vectors.

Going back to our fourier series,
\be
  f(\theta) = \sum_{n = 0}^{\infty} A_n \cos(n \theta) + B_n \sin(n \theta)
\ee
now that we can think of having an infinite vector space where our functions are vectors, we can think of this series as a sum of normalized basis vectors in the form of sine and cosine with weightings $A_n$ and $B_n$.
If we use dirac notation, we can make our series look like
\be
  \ket{f(\theta)} = \sum_{n=0}^{\infty} A_n \ket{c_n} + B_n \ket{s_n}
\ee
where the $\ket{c_n}$'s are our cosines with integer n and the $\ket{s_n}$'s are our sines with integer n.

Since we now have a vector space, we want to be able to define an inner product (you may remember it as the dot product) on our vector space.
The reason why we want to define an inner product is because we want to be able to define an orthogonal projection of one vector onto another.
When we project a vector $\vec{u}$ onto another vector $\vec{v}$, we consider this as taking the dot product
\be
  \vec{u} \cdot \vec{v} = u_1v_1 + u_2v_2 + \hdots
\ee
and in dirac notation this looks like
\be
  \vec{u} \cdot \vec{v} = \braket{u|v}
\ee
And since our functions can be thought of as vectors, we can do the same thing!
But if you're confused about how to do that, it turns out that taking an integral is exactly the same thing as the dot product!
When we have two vectors, the dot product is defined as multiplying a component from one vector and a component from another vector and adding up all the multiplications.
When we have two functions f and g, the integral of their product is defined as multiplying a function evaluated at one point and another function evaluated at a point where each component has a weight dx and adding up all the multiplications.
\be
  \int_a^b f(x)g(x) = f(a)g(a)dx + f(a + dx)g(a + dx)dx + \hdots
\ee
So our orthogonal projection is the same thing as taking the integral of the product of our orthogonal functions!
\be
  \braket{f|g} = \int_a^b f^*(x)g(x) dx
\ee
Notice that the function in the ket needs to be in the form of its complex conjugate.
If our sines and cosines are real functions however, then the complex conjugate of a real function is simply itself.

Now that we know we can take dot products of our series with other vectors, we can use our vectors' orthogonality properties to find an expression for the coefficients $A_n$ and $B_n$.
If we take the dot product of the cosine vector $\bra{c_n}$ with our $\ket{f(\theta)}$ vector, we get, in dirac notation,
\be
  \braket{c_n|f(\theta)} = \sum_{n=0}^{\infty} A_n \braket{c_n|c_n} + B_n \braket{c_n|s_n}
\ee
and since we know from equation \ref{eq:sin_cos} that the integral of sine and cosine is always zero, then the dot product $\braket{c_n|s_n}$ is always zero.
And since we know from equation \ref{eq:cos_cos} that the integral of cosine and cosine is 1 (don't forget the normalization constants) when $n = m$ and 0 when $n \neq m$, then all the terms in the sum but one will cancel out and all that we're left with is
\be
  A_n = \braket{c_n|f(\theta)} = \frac{1}{\sqrt{\pi}} \int_0^{2\pi} \cos(n \theta) f(\theta) d\theta
\ee
If we take the dot product of the sine vector $\bra{s_n}$ with our $\ket{f(\theta)}$ vector, we get, in dirac notation
\be
  \braket{s_n|f(\theta)} = \sum_{n=0}^{\infty} A_n \braket{s_n|c_n} + B_n \braket{s_n|s_n}
\ee
And using the same logic as before, equation \ref{eq:sin_cos} says that $\braket{s_n|c_n}$ is zero for any n and m, and equation \ref{eq:sin_sin} says $\braket{s_n|s_n}$ will only be 1 when $n = m$, so we're left with
\be
  B_n = \braket{s_n|f(\theta)} = \frac{1}{\sqrt{\pi}} \int_0^{2\pi} \sin(n \theta) f(\theta) d\theta
\ee
If you check what the Math methods book has for the coefficients $A_n$ and $B_n$, you'll notice that there is a factor of $\frac{1}{\pi}$ in front of the integrals, whereas we have $\frac{1}{\sqrt{\pi}}$.
The reason for this is because (and most books do not mention this explicitly) the book is choosing the convention where the basis functions are not normalized, whereas we have chosen to use the convention where the basis functions are normalized.
If you're starting to think that our convention is wrong, watch what happens when we explicitly write the fourier series with the coefficients written in their integral form using our convention.
\be
  \begin{split}
    f(\theta) &= \sum_{n = 0}^{\infty} \Big( \frac{1}{\sqrt{\pi}} \int_0^{2\pi} \cos(n \theta) f(\theta) d\theta \Big) \frac{1}{\sqrt{\pi}} \cos(n \theta) + \Big( \frac{1}{\sqrt{\pi}} \int_0^{2\pi} \sin(n \theta) f(\theta) d\theta \Big) \sin(n \theta) \\
    &= \sum_{n = 0}^{\infty} \Big( \frac{1}{\pi} \int_0^{2\pi} \cos(n \theta) f(\theta) d\theta \Big) \cos(n \theta) + \Big( \frac{1}{\pi} \int_0^{2\pi} \sin(n \theta) f(\theta) d\theta \Big) \sin(n \theta)
  \end{split}
\ee
The Fourier series coefficient end up becoming exactly as the book has them written.
So both conventions give the same result.
An issue that occurs in scientific computing is that some math software packages will program Fourier series such that there are no $\sqrt{\pi}$'s or $\pi$'s at all and you are free to add them wherever you like.
This becomes a third convention that simply does not make any sense mathematically and can cause confusion if you come across these packages.

This whole time, we've defined a Fourier series in the interval $0 \leq \theta \leq 2 \pi$ and we would now like to define a Fourier series that is periodic in an interval of any length L.
So if we wanted our Fourier series to go from representing functions that are periodic with some period $2\pi$ in the variable $\theta$ to representing functions that are periodic with some period L in the variable x, we let
\be
  x = \frac{\theta L}{2 \pi}
\ee
where $\frac{L}{2 \pi}$ can be thought of as the contraction/expansion ratio that is multiplied by our length $\theta$ to get our new length x.
If we rearrange the equation above we get
\be
 \theta = \frac{2 \pi x}{L}
\ee
and our Fourier series is now
\be
 f(\theta) = \sum_{n = 0}^{\infty} A_n \cos(\frac{2 \pi n x}{L}) + B_n \sin(\frac{2 \pi n x}{L})
\ee
As for our coefficients, we do not need to change the $\frac{1}{\sqrt{\pi}}$ because that comes naturally from our normalized basis.
But we need to make the substitution
\be
  \theta = \frac{2 \pi x}{L} \qquad d\theta = \frac{2 \pi}{L} dx
\ee
So our $A_n$ becomes
\be
  \begin{split}
    A_n &= \braket{c_n|f(x)} \\
    &= \frac{1}{\sqrt{\pi}} \frac{2 \pi}{L} \int_0^{L} \cos \Big( \frac{2 n \pi x}{L} \Big) f(x) d\theta \\
    &= \frac{2 \sqrt{\pi}}{L} \int_0^{L} \cos \Big( \frac{2 n \pi x}{L} \Big) f(x) d\theta \\
  \end{split}
\ee
and our $B_n$ becomes
\be
  \begin{split}
    B_n &= \braket{s_n|f(x)} \\
    &= \frac{2 \sqrt{\pi}}{L} \int_0^{L} \sin \Big( \frac{2 n \pi x}{L} \Big) f(x) d\theta \\
  \end{split}
\ee
which is now defined on an interval of length L such as $0 \leq x \leq L$, etc.
Again, the book defines the coefficients with a $\frac{2}{L}$ but I can assure you that if you plug in our coefficients like I did before, both conventions will yield the same series.

Now that we have a full understanding of where a Fourier series comes from, we can consider some examples that require us to create one.
However, we basically covered how to create a Fourier if we are given $f(\theta)$ or $f(x)$ for any periodic function.
Therefore, we will consider some special cases where our given function is a periodic even or odd function.

\end{document}
