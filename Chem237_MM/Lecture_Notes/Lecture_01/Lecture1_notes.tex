\documentclass{article}
%==============================================================================%
%	                          Packages                                     %
%==============================================================================%
% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\usepackage[margin=0.7in]{geometry}
\usepackage[version=4]{mhchem}
%==============================================================================%
%                           User-Defined Commands                              %
%==============================================================================%
% User-Defined Commands
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\pd}{\partial}
\newcommand{\dg}{\dagger}
%==============================================================================%
%                             Title Information                                %
%==============================================================================%
\title{Chem237: Lecture 1}
\date{4/5/18}
\author{Shane Flynn, Alan Robledo}
%==============================================================================%
%	Everyone Please Make Comments if Something Needs to be Reviewed        %
%                           Or just fix it yourself!                           %
%==============================================================================%
\begin{document}
\maketitle
\section*{Course Overview}
The course will not directly follow the textbook, however, it will provide a rough guideline for important topics.
The lecture and homework are the most important aspects of the course.
The homework will be split into both analytical and numerical problems, any programming language is fine for the numerical problems, however, mathematica and other highly developed languages are discouraged.
For the numerical problems if you are asked to make an algorithm you CANNOT use pre-built algorithms available to the languages.

\section*{Chapter 1: Differential Equations}
A Differential Equation simply refers to an equation containing derivatives.
An \textbf{Ordinary Differential Equation} (ODE) refers to any differential equation with derivatives taken wrt a single variable.
In contrast a \textbf{Partial Differential Equation} (PDE) contains derivatives wrt more than 1 variable.
We can further categorize differential equations (both ODE and PDE) into linear or nonlinear.
As you can imagine, linear equations are simpler to solve compared to nonlinear and ODEs are easier to solve than PDEs.

\subsection*{Linear Equations}
Linear Equations are a general topic, there are linear differential equations, linear algebraic equations, and etc.
A linear equation has the form
\be
\hat{A} f(x,y,\hdots) = B(x,y,\hdots)
\ee
Where the RHS (B) is known, and the function f is an unknown function.
To be a linear equation, the operator $\hat{A}$ must be a linear operator.
For example, take the equation above and let $\hat{A}$ be the differential operator wrt a single dimension, and you get a linear ordinary differential equation.

If the RHS (B) is equal to 0, the linear equation is called \textbf{Homogeneous} and if the RHS is anything other than 0, the linear equation is called \textbf{Inhomogeneous}.

\subsubsection*{Homogeneous Linear Equations}
Consider equation \ref{eq:lin_homo} below
\be \label{eq:lin_homo}
\hat{A}f = 0
\ee
If we assume f$_1$ and f$_2$ are solutions to equation \ref{eq:lin_homo}, then we immediately know another solution to be a linear combination of f$_1$ and f$_2$
\be
f_3 = c_1f_1 + c_2f_2 .
\ee
This result is true for any set of linear homogeneous equations.
The linear combination of two unique solutions to a linear homogeneous equation gives another unique solution.

\subsubsection*{Inhomogeneous Linear Equations}
If we now consider the Inhomogeneous linear equation, equation \ref{eq:lin_inhomo}
\be \label{eq:lin_inhomo}
\hat{A}f = B
\ee
we can construct the solution to this equation as
\be
f = f_0 + \sum_i c_i f_i
\ee
Where the specific solution ($f_0$) is known as a \textbf{Particular Solution} and the $f_i$ solutions refer to the solutions of the associated homogeneous equation.

\subsection*{ODE: Linear, Homogeneous, Constant Coefficients}
We will start with the simplest ODEs, the linear, homogeneous, constant coefficient expressions.
Various books use different notation for presenting ODEs.
For example, it is very common in physics to consider functions that only depend on time ==$>$ x = x(t).

We can also define notation as
\be
x^n \equiv \frac{d^n}{dt}x \equiv D^n x
\ee
Hopefully it is clear that D is the linear differential operator.
If you apply a linear operator n times it is still linear therefore D$^n$ is also a linear operator.

The general form of these equations is
\be \label{eq:lin_cc}
\sum_{n=0}^N a_nx^{(n)} = f(t)
\ee

If we consider the homogeneous linear constant coefficients, than equation \ref{eq:lin_cc} simplifies to
\be \label{eq:lin_cc_homo}
\sum_{n=0}^N a_nx^{(n)} = 0
\ee
To solve these equations, we assume a solution of the form: x(t) = e$^{\lambda t}$ and substitute this into equation \ref{eq:lin_cc_homo}.
\be
\begin{split}
    \sum_{n=0}^N a_nx^{(n)} &= 0\\
    \sum_{n=0}^N a_n \left(e^{\lambda t}\right) ^{(n)} &= 0\\
    \sum_{n=0}^N a_n \lambda^n e^{\lambda t} &= 0\\
    e^{\lambda t} \sum_{n=0}^N a_n \lambda^n &= 0\\
    \sum_{n=0}^N a_n \lambda^n &= 0\\
\end{split}
\ee

The last line is known as the \textbf{Characteristic Polynomial} and it helps us find the general solution to the linear constant coefficient ODE.
The above polynomial is of degree N, and a theorem in  ODEs says that there exists exactly N roots (in general complex) that can potentially be degenerate.
%==============================================================================%
% Someone go find this theorem
%==============================================================================%

The reason for using this approach comes solely from mathematical intuition.
Our differential equation requires a function $x(t)$ such that the function and its derivatives cancel out to satisfy the RHS.
The only function we know of that satisfies this condition is x(t) = e$^{\lambda t}$.
In order to account for different multiples of the function, we include the $\lambda$ in our assumed solution, since its derivative is equal to $\lambda$ e$^{\lambda t}$.
One would think that trigonometric functions and several derivatives of itself can satisfy this condition as well.
While this is sometimes true, it is not true generally.
Even if you consider a combination of sine and cosine, such a function would not always work.
Consider the example ODE below,
\be
\frac{dy}{dt} + y = 0 .
\ee
Suppose that we assume the solution to be x(t) = sin($\lambda$ t) + cos($\lambda$ t).
Plugging this and its derivative into the original equation yields
\be
\lambda cos(\lambda t) - \lambda sin(\lambda t) + sin(\lambda t) + cos(\lambda t) = 0 .
\ee
However, there are no values of $\lambda$ that could satisfy this equation.
Therefore, we are only left with generally assuming that the solution to linear constant coefficient ODEs is an exponential function to avoid situations similar to the above example.

We begin exploring situations of non-degenerate and degenerate roots of the characteristic equation by assuming the non-degenerate case (i.e. $\lambda_i \neq \lambda_j$).
Since each root of the polynomial gives us the $\lambda_n$ in the general solution, we can write the general solution to the ODE as
\be
x(t) = \sum_{n=1}^N c_n e^{\lambda_n t}
\ee
Finding the general solution for the degenerate case is not as straightforward because if we have two roots that are the same (e.g. $\lambda_1$ = $\lambda_2$), we would not be able to form 2 unique solutions from these roots.
We would only be able to form one unique solution.
However, there are multiple ways to show that for each repeated root, each solution will contain some multiple of the variable in question.
For example, if we had a 3rd order linear ODE with repeated roots, $\lambda_1$ = $\lambda_2$ = $\lambda_3$, the general solution would be
\be
x(t) = c_1 e^{\lambda_1 t} + c_2 t e^{\lambda_1 t} + c_3 t^2 e^{\lambda_1 t} .
\ee
There are 2 ways to show that this solution holds.
Consider the following second order ODE with constant coefficients
\be \label{eq:lin_cc_homo_reproot}
a\frac{d^2 y}{dt^2} + b\frac{d y}{dt} + cy = 0
\ee
If the above ODE has two repeated roots, $\lambda_1$ and $\lambda_2$, we can say that one unique solution to the ODE is
\be
y_1(t) = c_1 e^{\lambda_1 t}
\ee
We can consider evaluating the following limit to obtain the resulting second unique solution
\be
\lim_{\lambda_1\to\ \lambda_2} \frac{e^{\lambda_1 t} - e^{\lambda_2 t}}{\lambda_2 - \lambda_1} = t e^{\lambda_2 t} .
\ee
To show the second way, we again consider equation \ref{eq:lin_cc_homo_reproot}.
Since one unique solution is $c_1 e^{\lambda_1 t}$, we can replace the constant $c_1$ with a function $v(t)$ and try to determine the function $v(t)$ such that it is also a solution of equation \ref{eq:lin_cc_homo_reproot}.

So the linear constant coefficient ODE problem is equivalent to finding the roots of a polynomial, and related to the linear algebra problem (which we will see later in the course).
%==============================================================================%
% Someone try making the ODE subsection flow better
%==============================================================================%

\subsubsection*{Damped Harmonic Oscillator}
Let's consider an example, the \textbf{Damped Harmonic Oscillator}; a very common physics example with analytic solutions.
\be
\frac{d^2}{dt^2} x + 2\gamma \frac{d}{dt} x + \omega_o^2 x = 0
\ee
The $\gamma$ is interpreted as the damping constant, and the $\omega_o$ term is interpreted as the frequency of oscillations if no damping was present, aka the natural frequency.
While we do not live in an imaginary world, we would prefer to generalize $x(t)$ by making it a complex function.
If we ever need to compute real observables of the oscillator, this can be done by just taking the real part of the general solution.
Now, if we assume ==$>$ x = e$^{\lambda t}$ we can find the characteristic equation.
\be
\lambda^2 + 2\gamma\lambda  + \omega^2 = 0\\
\ee
Since the ODE is second order, we can predict that the characteristic equation will be quadratic with two roots $\lambda_+$ and $\lambda_-$
\be \label{eq:oscillator_char}
\lambda_{\pm} = -\gamma \pm \sqrt{\gamma^2 - \omega_o^2} .
\ee

In order to observe the behavior of the oscillator (i.e. solve the differential equation), we need to consider 3 different cases:
\benum
\item $\omega > \gamma$, the underdamped solution.
\item $\omega < \gamma$, the overdamped solution.
\item $\omega = \gamma$, the degenerate solution ($\lambda_1 = \lambda_2$).
\eenum

\subsubsection*{Underdamping ($\omega_o > \gamma$)}
In the case of underdamping, the damping constant is small compared to $\omega_o$. In this case, the square root in equation \ref{eq:oscillator_char} is imaginary and we can say
\be
\sqrt{\gamma^2 - \omega_o^2} = i \Omega = i \sqrt{\omega_o^2 - \gamma^2}
\ee
Therefore,
\be
\lambda_{\pm} = -\gamma \pm i \Omega .
\ee
And we can write the general solution as
\be
\begin{split}
x(t) &= c_1 e^{(-\gamma + i \Omega)t} + c_2 e^{(-\gamma - i \Omega)t} \\
&= e^{-\gamma t}(c_1 e^{i \Omega t} + c_2 e^{-i \Omega t}) \\
\end{split}
\ee
Here c$_1$ and c$_2$ can be complex.
The term within parenthesis is an oscillaiting component, and the outside terms shows an expoetential decay. 

Now that we have found the general solution, we would like to plot the position as a function of time. Since it is difficult to visualize in its current form, let's rewrite it in terms of sine and cosine using euler's identity
\be
\begin{split}
x(t) &= e^{-\gamma t} \big[ c_1(\cos(\Omega t) + i \sin(\Omega t)) + c_2(\cos(\Omega t) - i \sin(\Omega t)) \big] \\
x(t) &= e^{-\gamma t} \big[ (c_1 + c_2)\cos(\Omega t) + i(c_1 - c_2)\sin(\Omega t) \big] \\
\end{split}
\ee
and introduce new coefficients $B_1$ and $B_2$
\be
\begin{split}
&B_1 = c_1 + c_2 \\
&B_2 = i(c_1 - c_2) . \\
\end{split}
\ee
This turns the general solution into
\be
x(t) = e^{-\gamma t} \big[ B_1 cos(\Omega t) + B_2 sin(\Omega t)]
\ee

Since we know that $x(t)$ is real, along with $\cos(\Omega t)$ and $\sin(\Omega t)$, $c_1$ and $c_2$ must be chosen carefully to ensure that $B_1$ and $B_2$ are real. To continue reducing the solution to a more manageable form, we need to introduce one last variable
\be
A = \sqrt{B_1^2 + B_2^2}
\ee
where B$_1$ and B$_2$ can be thought of as the adjacent and opposite sides of a right triangle, respectively, with A as the hypotenuse and $\delta$ as the lower angle.
With that, we can use some geometry to rewrite the general solution as
\be
\begin{split}
x(t) &= Ae^{-\gamma t} \Big[\frac{B_1}{A}cos(\Omega t) + \frac{B_2}{A}sin(\Omega t) \Big]\\
 &= Ae^{-\gamma t} [cos(\delta)cos(\Omega t) + sin(\delta)sin(\Omega t)]\\
 &= Ae^{-\gamma t} cos(\Omega t - \delta)\\
\end{split}
\ee
where we used the trig identity $cos(A)cos(B) + sin(A)sin(B) = cos(A-B)$.

We now have a very nice and compact equation to describe the motion of the oscillator.
%==============================================================================%
% We should probably add the plot of the solution right here.
% Make sure we add the plot, Real component of x(t) vs t, also plot the expotential decay e^{-gamma t} to compare with
% Allplots should be written in a programming language and privided on the github page too, use whatever language you like
% If you need extra packages to plot (i.e. pandas for python or something) make a note in the src code
%==============================================================================%
By looking at the solution in this form, we can see that the oscillatory motion comes from the cosine term and the oscillator tending towards equilibrium comes from the negative exponential term.

\subsubsection*{Overdamping ($\omega_o < \gamma$)}
The overdamping case can be physically interpreted as having a very large friction contribution. 
In the case of overdamping, the square root in equation \ref{eq:oscillator_char} is real and not complex, so we can just write the general solution as
\be
\begin{split}
x(t) &= c_1 e^{-\gamma + \sqrt{\gamma^2 - \omega_o^2}} + c_2 e^{-\gamma - \sqrt{\gamma^2 - \omega_o^2}} \\
&= c_1 e^{-(\gamma - \sqrt{\gamma^2 - \omega_o^2})} + c_2 e^{-(\gamma + \sqrt{\gamma^2 - \omega_o^2})}
\end{split}
\ee
%==============================================================================%
% I wrote equation 28 this way to bypass using Vlad's upside down plus minus thing.
%==============================================================================%
The reason for rewriting the general solution is to see more clearly what is happening to the oscillator's position by noticing the two negative exponential terms.
In the case of overdamping, the resistive force, proportional to the damping constant, dominates the spring force.
So once the oscillator is kicked at time t $=$ 0, it will move to its maximum displacement and then quickly decay to zero displacement without anymore oscillations.
%==============================================================================%
% Add the other plot of the solution here.
% Again plot Re[x(t) vs t]
%==============================================================================%

\subsubsection*{Critical Damping ($\omega_o = \gamma$)}
%==============================================================================%
% Need to aelaborate/understand the motivation for this solution somehow
%==============================================================================%
In the case of critical damping, the damping constant is equal to the natural frequency.
$\lambda_1 = \lambda_2 = \omega = \gamma$. 
To find the general solution let's first assume we do not have a degenerate solution.
In this case we would have $\lambda_1 \neq \lambda_2$ and $e^{\lambda_1 t} - e^{\lambda_2 t}$. 
We can take this solution and multiply or divide by any number and still have a solution. 
So we consider a solution of the form
\be
\frac{e^{\lambda_1 t} - e^{\lambda_2 t}}{\lambda_1-\lambda_2}
\ee
We can now consider the limit as we approach our degenerate case (i.e. $\lambda_1=\lambda_2$). 
%==============================================================================%
% Need to do this limit
%==============================================================================%
\be
\lim_{\lambda_1\to\lambda_2}  \frac{e^{\lambda_1 t} - e^{\lambda_2 t}}{\lambda_1-\lambda_2} = te^{\lambda t}
\ee
And the general solution can be written as 
\be
x(t) = (A + Bt)e^{-\lambda t}
\ee
With arbitrary cinstants A and B. 
%==============================================================================%
% ALan need to merge the above (from class) with your notes below
%==============================================================================%
This means that the characteristic equation has two equal roots and the general solution can be written as
\be
\begin{split}
x(t) &= c_1 e^{\lambda_1 t} + c_2 te^{\lambda_2 t}\\
&= c_1 e^{-\gamma t} + c_2 te^{-\gamma t}\\
\end{split}
\ee

Here we see that our ODE is second order, and we get 2 free (potentially complex) parameters, meaning the dimensionality of the solution space is 2. 
In general an N$^{th}$ order equation, assuming degenerate roots
\be
\sum_{n=0}^N a_n x^{(n)} = 0 \xrightarrow{\text{degenerate}} \sum_{n=0}^N a_n \lambda^n = 0
\ee
In the degenerate case this maps to
\be
\sum_{n=0}^N a_n \lambda^n = 0 \Leftrightarrow (\lambda - \lambda_1)^{k_1} (\lambda - \lambda_2)^{k_2} \cdots = 0
\ee
With a general solution of 
\be
x(t) = \left( A_0 + A_1t + A_2t^2 + \cdots + A_{k_1} t^{k_1}\right) e^{\lambda_1t} \left( B_0 + B_1t + B_2t^2 + \cdots + B_{k_2} t^{k_2}\right) e^{\lambda_2t}
\ee
Where there are N free paramaters such that $k_1 + k_2 + \cdots = N$. 

\subsection*{Inhomogeneous Linear ODE (constant Coefficients)}
The general form for an inhomogeneous equation with constant coefficiemts is given by (followed by the associated homogenous equation)
\be
\begin{split}
	\sum_{n=0}^N a_n x^{(n)} &= f(t)\\
	\sum_{n=0}^N a_n x^{(n)} &= 0
\end{split}
\ee
This equation is well known, for example the n=2 case yields the \textbf{Driven Harmonic Oscillator}.
The general solution for this problem is of the form
\be
x(t) = x_0(t) + \sum_{n=1}^N c_nx_n(t)
\ee
Where x$_0$(t) is a \textbf{Particular Solution} to the inhomogeneous equaiton, and x$_n$(t) = e$^{\lambda_n t}$. 
Unfortunately there is no general method to solve for the particular solution.
But there are special cases where a solution can be found.
Unfortunately all of these special cases require some type of insight (a trick) to solve.

\subsubsection*{Case 1}
If we have f(t) = $Be^{\Omega t}$ and we assume the solution to be of the form $x_0(t) = Ae^{\Omega t}$.
Substituting this guess into the into the inhomogeneous equation we find
\be
A \sum_{n=0}^N a_n \omega^n B \Rightarrow A = \frac{B}{\sum_{n=0}^N a_n \omega_n}
\ee
At no point did we assume $\Omega \in \Re$, so this derivation is in general complex.

\subsubsection*{Case 2}
Our next special case is of the form $f(t) = B_1e^{\Omega_1t} + B_2e^{\Omega_2t} + \cdots$
We will guess a solution of the form $x_0(t) = A_1 e^{\Omega_1t} + A_2 e^{\Omega_2t} + \cdots$
Substituting this into the inhomigeneous equation we find 
\be
e^{\omega_1t} A_1
\ee




\subsubsection*{Case 3}


\subsubsection*{Case 4}






\end{document}
