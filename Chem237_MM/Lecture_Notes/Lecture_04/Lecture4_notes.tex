\documentclass{article}
%==============================================================================%
%	                          Packages                                     %
%==============================================================================%
% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\usepackage[margin=0.7in]{geometry}
\usepackage[version=4]{mhchem}
%==============================================================================%
%                           User-Defined Commands                              %
%==============================================================================%
% User-Defined Commands
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\pd}{\partial}
\newcommand{\dg}{\dagger}
\newcommand{\sumzero}{\sum_{n=0}^\infty}
\newcommand{\sumone}{\sum_{n=1}^\infty}
%==============================================================================%
%                             Title Information                                %
%==============================================================================%
\title{Chem237: Lecture 4}
\date{4/10/18}
\author{Alan Robledo, Shane Flynn}
%==============================================================================%
%	Everyone Please Make Comments if Something Needs to be Reviewed        %
%                           Or just fix it yourself!                           %
%==============================================================================%
\begin{document}
\maketitle

\subsection*{Power Series}
In general, a power series is an infinite series of the form
\be
\sumzero a_n x^n = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \hdots
\ee
where the a$_n$'s are called the coefficients of the series.
The sum can be identified as a function of infinte degree of x whose domain is the set of all x for which the series converges.
For exmample, last lecture, we saw that if a$_n = 1$, we obtain the geometric series
\be
\sumzero x^n = 1 + x + x^2 + x^3
\ee
and the domain of the series is $-1 < x < 1$.
In other words, the geometric series only converges for values of x greater than -1 and less than 1.
A more general power series, an infinite series of the form
\be
\sumzero a_n (x - x_o)^2 = a_0 + a_1 (x - x_o) + a_2 (x - x_o)^2 + \hdots
\ee
is called a power series centered about x$_o$.
Notice that if x$_o$ = 0, then you get back the power series mentioned earlier.
Also notice that the term $(x - x_o)^0$ is equal to 1 for all values of x, including x = x$_o$ (even though you have been told at some point in your algebra class that $0^0$ is undefined).

In physics, functions are often written in their power series representations by way of a \textbf{Taylor series} expansion about a point x$_o$, with x$_o = 0$ typically being the easiest value to expand a function about.
A Taylor series is a special kind of power series because it is computed by evaluating the function's derivatives at a single point x$_o$.
Formally, this is written as
\be \label{eq:general_taylor}
\sumzero \frac{f^{(n)} (x_o)} {n!} (x - x_o)^n = \frac{f(x_o)}{0!} + \frac{f'(x_o)}{1!} (x - x_o) + \frac{f''(x_o)}{2!} (x - x_o)^2 + \hdots + \frac{ f^{(n)} }{n!}(x - x_o)^n + \hdots
\ee
where $f^{(n)} (x_o)$ denotes the n-th derivative of the function evaluated at the center x$_o$.
Again, we typically set x$_o = 0$ and most expansions of functions you find online use this value as the center as well, such as
\be
\frac{1}{1-x} = 1 + x + x^2 + x^3 + \hdots = \sumzero x^n
\ee
\be
e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \hdots = \sumzero \frac{x^n}{n!}
\ee
\be
\cos(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \hdots = \sumzero \frac{(-1)^n x^{2n}}{(2n)!}
\ee
\be
\ln(1+x) = x - \frac{x^2}{2!} + \frac{x^3}{3!} - \hdots = \sumzero (-1)^{n} \frac{x^n}{n}
\ee
For those who do not remember the process of obtaining a Taylor series, we can consider Taylor expanding $\frac{1}{1 - x}$, since we were able to show that the infinite limit of the partial sum of the geometric series converged to $\frac{1}{1-x}$ in lecture 3.
The first thing you do is compute the first few derivatives and then evaluate them at the center point which we will take to be zero.
\be
\begin{split}
f^{(0)} (x_o = 0) &= \frac{1}{1-0} = 1 = 0!\\
f^{(1)} (x_o = 0) &= \frac{1}{(1-0)^2} = 1 = 1!\\
f^{(2)} (x_o = 0) &= \frac{2}{(1-0)^3} = 2 = 2!\\
f^{(3)} (x_o = 0) &= \frac{6}{(1-0)^4} = 6 = 3!\\
\end{split}
\ee
and then we plug them into equation (\ref{eq:general_taylor}) to obtain our series
\be
\frac{1}{1-x} = f(0) + \frac{f'(0)}{1!} (x - 0) + \frac{f''(0)}{2!} (x - 0)^2 + \frac{f'''(0)}{3!} (x - 0)^3 + \hdots = 1 + x + x^2 + x^3 + \hdots = \sumzero x^n
\ee
You can try taylor expanding the functions in equations 5-8 about the point x$_0$ = 0 to see if you actually do get the infinite series shown.

Physics often uses Taylor series expansions because a function can be approximated by simply truncating your series to any degree in x that you want.
As you keep higher and higher order terms, your approximate equation will yield more accurate values, with the infinite degree polynomial being the exact equation.
So it is very reasonable to represent the function sin($\theta$) as a first degree polynomial in $\theta$ if the value of $\theta$ is reasonably small.
\be
\sin{\theta} = \theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \hdots \approx \theta
\ee
And this approximation, known as the small-angle approximation, is almost always used when considering oscillating systems where the angle of oscillations is very small because the math becomes much easier to handle when the substitution is made.

\subsubsection*{Manipulating Series}
Now that we are aware of how to create an infinite series out of any function, we want to become familiar with how to manipulate a series to give you another one.
If we are given a series we do not recognize, we can always try applying a transformation to turn it into a recognized form, such as taking a derivative or an integral of the original series.
Consider the general power series
\be
f(x) = \sumzero a_nx^n
\ee
We can write a new series by taking the derivative with respect to x
\be
f'(x) = \sumzero na_nx^{n-1}
\ee
or integrating with respect to x
\be
\int_{0}^{x} f(x) dx = \sumzero \frac{a_n}{n+1}x^{n+1}
\ee
Consider for example the following series
\be
\sumzero nx^n
\ee
If we wanted to find the function that the series equates to, we first notice that it looks similar to the derivative of the geometric series with respect to x
\be
	\sumzero x^n = \frac{1}{1-x} \Rightarrow \frac{d}{dx} \left( \sumzero x^n\right) = \frac{d}{dx} \left( \frac{1}{1-x} \right) = \frac{1}{(1-x)^2} = \sumzero nx^{n-1}
\ee
To make this last line look like our example series, we just need to multiply by x
\be
x \sumzero nx^{n-1} = \sumzero nx^{n} = \frac{x}{(1-x)^2}
\ee
As another example, consider
\be
\sumone \frac{x^n}{n}
\ee
Again, this expression looks similar to a geometric series but if you look carefully, you will see that the geometric series is evaluated from zero to infinity while this series is evaluated from 1 to infinity.
This means that we will need to perform an \textbf{index shift}.
The process is simply changing the value that the series starts froms.
Obviously, we can't just change the $n = 0$ to $n = 1$ because we would be changing the value of the sum.
To prevent from changing the value of the sum, we first define a new index
\be
m = n - 1
\ee
and now, when $n = 1$, our new index $m = 0$.
Since we are dealing with infinte sums, we don't have to worry about the upper limit because $\infty - 1 = \infty$.
And now, we can rewrite our sum in terms of our new index by solving for n
\be
n = m + 1
\ee
giving us
\be
\sumone \frac{x^n}{n} = \sum_{m = 0}^{\infty} \frac{x^{m + 1}}{m + 1}
\ee
and we can re-write our new sum in terms of our original index n since the value of the sum is not dependent on the variable we choose to represent the index
\be
\sumone \frac{x^n}{n} = \sum_{n = 0}^{\infty} \frac{x^{n + 1}}{n + 1}
\ee
And the sum can be obtained by just integrating the geometric series
\be
\int_{0}^{x} \sumzero x^n = \int_{0}^{x} \left( \frac{1}{1 - x} \right) dx \Rightarrow \sum_{n = 0}^{\infty} \frac{x^{n + 1}}{n + 1} =  - \ln(1 - x) = \sumone \frac{x^n}{n}
\ee
Going back to the Reimann Zeta function, we can consider what was known in the mid-1600s as the Basel problem, which consisted of trying to evaluate the Reimann Zeta function when s = 2
\be
\zeta(2) = \sum_{n=1}^\infty \frac{1}{n^2}
\ee
Sometime in the 1730s, Leonhard Euler was able to find out that the value of the sum was equal to $\frac{\pi^2}{6}$.
Multiple proofs exist that use theorems such as Parseval's theorem and the Residue theorem (which we'll see later in the course), but let's see if we can use what learned so far about evaluating series sums to evalaute this sum
Consider the function S(x)
\be
S(x) = \sum_{n=1}^\infty \frac{x^n}{n^2}
\ee
and S(1) would give us back our original sum
\be
S(1) = \sum_{n=1}^\infty \frac{1}{n^2}
\ee
If we can find a closed form expression for the function S(x), then all we need to do is plug in x = 1 and we can get the value of our sum. So let's start with taking a derivative.
\be
\begin{split}
	S(x) &= \sum_{n=1}^\infty \frac{x^n}{n^2}\\
	S'(x) &= \sum_{n=1}^\infty \frac{x^{n-1}}{n}\\
\end{split}
\ee
If we multiply the sum by x, we can get back a series that we've already dealt with before.
\be
\begin{split}
	x S'(x) &= \sum_{n=1}^\infty \frac{x^{n}}{n}\\
	x S'(x) &= \ln(1-x)\\
\end{split}
\ee
To find S(x), all we need to do now is divide by x and integrate from 0 to x.
\be
	S(x) = \int_0^x \frac{\ln(1-x)}{x} dx
\ee
Sadly, this integral cannot be evaluated analytically.
But at least we have converted an infinite series to an integral and this is a solution.
\section*{Methods Of Integration}
This is not a specific chapter in the book, we are just going to cover some simple classes of integrals and standard solutions.
A common theme for this course, there is no general method for solving integrals, and most integrals cannot be solve in terms of elementary functions.

\subsection*{Integration By Parts}
%==============================================================================%
%            Someone derive the IBP formula, comes fropm product rule of derivatives
%==============================================================================%
Integration By Parts is a common method for replacing one integral (you cannot solve) for another integral (that you can hopefully solvE).
It is useful if your integrand is a product of functions, and one of the functions can be reduced in order by taking a derivative.
\be
\int_a^b U(x) dV(x) = U(x) V(x) \bigg | _a^b - \int_a^b V(x) dU(x)
\ee

We do know that any elementary function has a derivative that is also an elementary function, so you could go about making a database for easily solvable problems.
Applying IBP then we want to solve for integrals that are the derivative of elementary functions.

As an example consider IBP of function I (containing parameter n).
\be
\begin{split}
	I(n) &= \int_a^b dx x^n d^x\\
	&= e^x x^n \bigg |_a^b - \int_a^b dx e^x n x^{n-1} \\
	&= e^x x^n \bigg |_a^b - n I(n-1)\\
	&= \cdots \\
	&= I(0) = \int_a^b dx e^x = e^x \bigg |_a^b
\end{split}
\ee
So for this example we can continue evaluating until we reach the integral of a simple expotential.
Therefore any expotential times a polynomial can be evalued using IBP.

If we can evaluate any polynomial times an expotential, consider another example.
\be
\int dx x^n \cos(x)
\ee
We know that we can express cos(x) in terms of expotentials
\be
\cos(x) = \frac{e^{ix}+e^{-ix}}{2}
\ee
So we can really convert this problem into the same form and solve with IBP, let s = ix
\be
\int ds x^n e^s
\ee
%==============================================================================%
%            Someone should do these integrals out as exmaples
%==============================================================================%

\subsection*{Gaussian Integrals}
Another well known and useful integral we will want to evaluate is the Gaussian.
\be
I = \int_{-\infty}^\infty dx e^{-x^2}
\ee
If the limits are not $\infty$ than you can get the special function known as the \textbf{Error Fucntion}.
Basically a special function is a common or useful function that appears but cannot be evaluated using elementary functions, it is useful wso we give it a name.
%==============================================================================%
%       May want ot add in comments about error functions since he mentioned it
%==============================================================================%
There is a well known trick for evaluating a Gaussian Integral.
You instead consider the square of the integral, and convert to polar coordinates to see a nice simplification.
\be
\begin{split}
	I &= \int_{-\infty}^\infty dx e^{-x^2} \Rightarrow \\
	I^2 &= \left(\int_{-\infty}^\infty dx e^{-x^2}\right) \left(\int_{-\infty}^\infty dy e^{-y^2}\right) \\
	 &= \int_{-\infty}^\infty \int_{-\infty}^\infty dxdy e^{-x^2 - y^2} \\
\end{split}
\ee
We can now switch over to polar coordinates and re-define the integrals
%==============================================================================%
%      Someone add in conversation reviewing polar coordinates
%==============================================================================%
\be
\begin{split}
	 I^2 &= \int_{-\infty}^\infty \int_{-\infty}^\infty dxdy e^{-x^2 - y^2} \\
	 &= \int_0^{2\pi} d\theta \int_0^\infty dr e^{0r^2}r \\
	 &= 2\pi \int_0^\infty dr re^{-r^2} \rightarrow[t=r^2]\\
	 I^2&= 2 \int_0^\infty dt e^{-t} = \pi \\
	 I &= \sqrt{\pi}
\end{split}
\ee

Consider now a related example
\be
I = \int dx e^x \sin(x)
\ee
We know that we can rewrite sin(x) as
\be
\sin(x) = \frac{e^{ix}-e^{-ix}}{2i}
\ee
If we make the substitution we find integrals of the form
\be
\int e^{ax} dx
\ee
Where a is potentially complex and we can work through the math.
%==============================================================================%
%      Someone can try to solve as a gaussian if they want
%==============================================================================%
However, we can also just use integration by parts to evaluate this integral.
%==============================================================================%
%      Someone show more algebra for the IBP
%==============================================================================%
\be
\begin{split}
	I &= \int dx e^x \sin(x) = -e^x \cos(x) + \int dx e^x \cos(x) \rightarrow[\text{IBP Again}]\\
	I &= -e^x \cos(x) + e^x \sin)x) - I = I = \frac{-e^x \cos(x) + e^x \sin(x)}{2}
\end{split}
\ee

\subsection*{Integrals of Rational Functions}
We can now consider anpother class of integrals.
Take P(x) to be a polynomial of degree n: P$_0$ + P$_1$x + $\cdots$ P$_n$x$^n$, and Q(x) to be a polynomial of degree m  Q$_0$ + Q$_1$x + $\cdots$ Q$_n$x$^n$.
\be
\int dx \frac{P(x)}{Q(x)}
\ee
To evaluate we will assume n $<$ m, which is always true because we can simply reduce by dividing by polynomials i.e. if m $>$ n than just divid the polynomials.

It is always possible to find the roots of a polynomial, can be done analytically if degree 4 or less, numerically if higher degree polynomials are involved.
SO it is safe to assume the roots exist, we will assume the roots are not degenerate for convenience.
This means Q can be written as Q(x) = (x-x$_1$) $\cdots$ (x-x$_m$).
\be
\frac{P(x)}{Q(x)} = \frac{P(x)}{(x-x_1)(x-x_2)\cdots(x-x_m)} = \frac{A_1}{x-x_1} + \cdots + \frac{A_m}{x-x_m}
\ee

%==============================================================================%
%      Someone check if it is partial fraction decomposition
%==============================================================================%
Where the coefficients A$_i$ can be found through% (partial fraction decomposition i believe it is called????). %
For example
%==============================================================================%
%      We need to check this vlad made it up on teh spot may be wrong
%==============================================================================%
\be
\frac{1}{(x-a)(x-b)} = \frac{A}{x-a} \frac{B}{x-b} \Rightarrow \frac{A(x-b) + B(x-a)}{(x-a)(x-b)} = 1
\ee
Doing some algebra we find
\be
A = \frac{1}{b-a}, \quad B = \frac{1}{b-a}, \quad \frac{1}{b-a} \left(\frac{1}{x-a} - \frac{1}{x-b}\right)
\ee
%==============================================================================%
%     I had this written kind of randomly not sure were it goes
%==============================================================================%
$\int \frac{A_k}{x-x_k} = A_k \ln(x-x_k)$
%==============================================================================%
%     I had this written kind of randomly not sure were it goes
%==============================================================================%

If we consider a different probelm with degenerate roots we find a similar result.
\be
\frac{1}{(x-a)^2(x-b)} = \frac{A_1x + A_0}{(x-a)^2} + \frac{B}{(x-b)}
\ee
And go through the process to find our new constants. another example.
\be
\frac{1}{(x-a)^2(x-b)^3} = \frac{A_1x + A_0}{(x-a)^2} + \frac{B_2x^2 + B_1x + B_0}{(x-b)^3}
\ee
And again we need to solve a system of equations to find the constants.

Consider a final strategy for evaluating integrals known as \textbf{Parameter Differentation}.
%==============================================================================%
%    Re-Do formatting, just wanted to finish q_q
%==============================================================================%
\be
I_0(a) = \int_0^\infty dx e^{-ax^2} = \frac{1}{2} \sqrt{\frac{\pi}{4}}
\ee
\be
I_1(a) = \int_0^\infty dx e^{-ax^2}x = \frac{1}{2a}
\ee
How do we solve I$_2$(a)?
Consider a derivative
\be
\frac{d}{da} I_0(a) = \int_0^\infty dx (-x^2) e^{-ax^2}
\ee

\be
I_2(a) = \int dx x^2e^{-ax^2} = \frac{1}{4a} \sqrt{\frac{\pi}{a}}
\ee

\be
I_{2n}(a) = \int dx x^{2n}e^{-ax^2} = \frac{1}{4a} \sqrt{\frac{\pi}{a}}
\ee

\be
\frac{d}{da} I_{2n}(a) \Rightarrow _{2n+1} a
\ee
The above is for even powers, and for odd powers we would apply.

\be
\frac{d}{da} I_1(a) \Rightarrow _{2n-1} a
\ee

\end{document}
