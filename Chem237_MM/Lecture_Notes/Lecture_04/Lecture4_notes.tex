\documentclass{article}
%==============================================================================%
%	                          Packages                                     %
%==============================================================================%
% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\usepackage[margin=0.7in]{geometry}
\usepackage[version=4]{mhchem}
%==============================================================================%
%                           User-Defined Commands                              %
%==============================================================================%
% User-Defined Commands
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\pd}{\partial}
\newcommand{\dg}{\dagger}
\newcommand{\sumzero}{\sum_{n=0}^\infty}
\newcommand{\sumone}{\sum_{n=1}^\infty}
%==============================================================================%
%                             Title Information                                %
%==============================================================================%
\title{Chem237: Lecture 4}
\date{4/10/18}
\author{Shane Flynn}
%==============================================================================%
%	Everyone Please Make Comments if Something Needs to be Reviewed        %
%                           Or just fix it yourself!                           %
%==============================================================================%
\begin{document}
\maketitle
\section*{More Series}
The following alternating series has some interesting convergence properties; it takes the form
\be
\sumone \frac{(-1)^{n+1}}{n} = \ln (1+1) = \ln (2)
\ee
If we look at some of the explicit values of the series, we see
\be
1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \cdots
\ee
In general, we know that addition and subtraction between numbers commute i.e. A + B = B + A, so we would expect that if we re-arrange the order in which we take our infinite summation, we would still produce the same result at the end.
So consider a fancy summation scheme for the alternating series where we group terms
\be
\left(1 + \frac{1}{3} + \frac{1}{5}\right) - \frac{1}{2} + \left(\frac{1}{7} + \frac{1}{9} + \frac{1}{11} + \frac{1}{13} + \frac{1}{15}\right) - \frac{1}{4} \cdots
\ee
So what we did was break the summation into positive and negative terms, defining a set of partial summations over the positive and then negative terms.
If you numerically evaluate the summations in this way, you will find that the summation appears to approach $\frac{3}{2}$ as n goes to $\infty$.
In fact it turns out that you can re-arrange the terms in any sort of pattern, and find the series converges to any value you want.
This is a consequence of the series not being absolutely convergent!
While the commutation of terms applies to finite numbers, it is not true for all infinite series, meaning A + B $\neq$ B + A over an infinite interval for a divergent or conditionally convergent series.
If the series is absolutely convergent however, than the series will converge to the same value no matter what, and changing the order of the summation will not affect the result.

\subsection*{Power Series}
The purpose of a power series is to expand a function in terms of x around a center point x$_o$.
We could expand any function around any center, but we usually consider x$_o = 0$ for simplicity.
This gives us the general power series centered around x$_o = 0$
\be
\sumzero a_x x^n = f(x)
\ee
Often times in physics, it is much easier to deal with functions when they are represented as a Taylor Series.
A Taylor series is a special kind of power series because such a series is computed by evaluating the values of the function's derivatives at a single point x$_o$.
Formally, this is written as
\be \label{eq:general_taylor}
\sumzero /frac{f^{(n)} (x_o)}{n!} (x - x_o)^n
\ee
where $f^{(n)} (x_o)$ denotes the n-th derivative of the function evaluated at the point x$_o$.
Again, we typically set x$_o = 0$ and most expansions of functions you find online use this value as well.
When x$_o = 0$, we call the series a Maclaurin series, but the name is not always used and such a series is still often referred to as a Taylor Series.
For example, we can expand the function $sin(x)$ as a Taylor series around x$_o = 0$ by first computing a few derivatives
\be
\begin{split}
f^{(0)} (x_o = 0) = \sin{0} = 0
f^{(1)} (x_o = 0) = \cos{0} = 1
f^{(2)} (x_o = 0) = - \sin{0} = 0
f^{(3)} (x_o = 0) = - \cos{0} = -1
\end{split}
\ee
Then we plug our variables into equation \ref{eq:general_taylor} to get a polynomial in x of an infinite degree
\be
\sin(x) = \sumzero /frac{(\sin{x})^{(n)} (x_o = 0)}{n!} (x)^n = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots
\ee
The reason why Taylor Series expansions are used often in physics is because the value of sin(x) can be approximated by simply truncating your series to any degree in x that you want.
As you keep higher and higher order terms, your approximate equation will yield more accurate values, with the infinite degree polynomial being the exact equation.
So it is very reasonable to represent the function sin(x) as a first degree polynomial in x if the value of x is reasonably small.
\be
\sin{x} \approx x
\ee
And this approximation is almost always used when considering oscillating systems where the angle of oscillations is very small because the math becomes much easier to solve when the substitution is made.
%==============================================================================%
%                             Put in derivation for the Taylor expansions.     %
%==============================================================================%

\subsubsection*{Geometric}
\be
\sumzero x^n = \frac{1}{1-x}
\ee

\subsubsection*{Expotential}
\be
\sumzero \frac{x^n}{n!} = e^x
\ee

\subsubsection*{Cosine}
\be
\sumzero \frac{x^{2k}(-1)^k}{(2k)!} = \cos(x)
\ee

\subsubsection*{Logarithm}
\be
\sumzero (-1)^{n+1} \frac{x^n}{n} = \ln(1+x)
\ee

Now that we are aware of how to create an infinite series out of any function, we want to become familiar with how to manipulate a series to give you another one.
If we are given a series we do not recognize, we can always try applying a transformation to turn it into a recognized form, such as taking a derivative or an integral of the original series.
Consider the general power series
\be
f(x) = \sumone a_nx^n
\ee
We can write a new series by taking the derivative with respect to x
\be
f'(x) = \sumone na_nx^{n-1}
\ee
or integrating with respect to x
\be
\int_{0}^{x} f(x) dx = \sums \frac{a_n}{n+1}x^{n+1}
\ee
Consider for example the following series
\be
\sums nx^n
\ee
If we wanted to find the function that the series equates to, we can notice that it looks similar to the derivative of the geometric series with respect to x
\be
\begin{split}
	\sum x^n = \frac{1}{1-x} &\Rightarrow \frac{d}{dx} \left( \sum x^n\right) = \frac{d}{dx} \left( \frac{1}{1-x} \right)\\
	\sum nx^{n-1} &= \frac{1}{(1-x)^2} \\
\end{split}
\ee
To make this last line look like our example series, we just need to multiply by x and change the summation bounds
\be
\sums nx^n = \frac{x}{(1-x)^2}
\ee
We can change the summation bounds from $n = 0$ to $n = 1$ because the 0th term is equal to zero, so it does not affect the summation.
As another example, consider
\be
\sums \frac{x^n}{n}
\ee
This expression again looks similar to a geometric series.
Consider what happens if we take a definite integral of the geometric series
\be
\sum x^n = \frac{1}{1-x} \Rightarrow \int_{0}^{x} \frac{1}{1-x} dx = - \ln(1 - x)
\ee

If we divide by x and then integrate, we can get the example series
\be
\begin{split}
	\int_{0}^{x} \left(\frac{1}{1-x}\right) dx &= \int \sum x^n dx = \sum \frac{x^{n+1}}{n} \Rightarrow \\
	\sum_{n=1}^\infty \frac{x^n}{n} &= \int \text{ } dx \left(\frac{1}{1-x}\right) \frac{1}{x} \Rightarrow \ln(1-x) \Rightarrow\\
	\sum_{n=1}^\infty \frac{x^n}{n} &= \ln(1-x)
\end{split}
\ee

Another example, we can start using power series, consider a function of x.
\be
S(x) = \sum_{n=1}^\infty \frac{x^n}{n^2}
\ee
Where S(1) would be evaluated as
\be
S(1) = \sum_{n=1}^\infty \text{ }\frac{1}{n^2}
\ee

The solution of S(x) is related to the previous problem, consider takng a derivative.
\be
\begin{split}
	S(x) &= \sum_{n=1}^\infty \frac{x^n}{n^2}\\
	S'(x) &= \sum_{n=1}^\infty \frac{x^{n-1}}{n}\\
	x S'(x) &= \sum_{n=1}^\infty \frac{x^{n}}{n}\\
	x S'(x) &= \ln(1-x)\\
	S'(x) &= \int_0^x dx \frac{\ln(1-x)}{x}
\end{split}
\ee
The integral on the last line cannot be evaluated analytically, but we have converted an infinite series to an integral which a nuch simplier problem, and this is a solution.

\section*{Methods Of Integration}
This is not a specific chapter in the book, we are just going to cover some simple classes of integrals and standard solutions.
A common theme for this course, there is no general method for solving integrals, and most integrals cannot be solve in terms of elementary functions.

\subsection*{Integration By Parts}
%==============================================================================%
%            Someone derive the IBP formula, comes fropm product rule of derivatives
%==============================================================================%
Integration By Parts is a common method for replacing one integral (you cannot solve) for another integral (that you can hopefully solvE).
It is useful if your integrand is a product of functions, and one of the functions can be reduced in order by taking a derivative.
\be
\int_a^b U(x) dV(x) = U(x) V(x) \bigg | _a^b - \int_a^b V(x) dU(x)
\ee

We do know that any elementary function has a derivative that is also an elementary function, so you could go about making a database for easily solvable problems.
Applying IBP then we want to solve for integrals that are the derivative of elementary functions.

As an example consider IBP of function I (containing parameter n).
\be
\begin{split}
	I(n) &= \int_a^b dx x^n d^x\\
	&= e^x x^n \bigg |_a^b - \int_a^b dx e^x n x^{n-1} \\
	&= e^x x^n \bigg |_a^b - n I(n-1)\\
	&= \cdots \\
	&= I(0) = \int_a^b dx e^x = e^x \bigg |_a^b
\end{split}
\ee
So for this example we can continue evaluating until we reach the integral of a simple expotential.
Therefore any expotential times a polynomial can be evalued using IBP.

If we can evaluate any polynomial times an expotential, consider another example.
\be
\int dx x^n \cos(x)
\ee
We know that we can express cos(x) in terms of expotentials
\be
\cos(x) = \frac{e^{ix}+e^{-ix}}{2}
\ee
So we can really convert this problem into the same form and solve with IBP, let s = ix
\be
\int ds x^n e^s
\ee
%==============================================================================%
%            Someone should do these integrals out as exmaples
%==============================================================================%

\subsection*{Gaussian Integrals}
Another well known and useful integral we will want to evaluate is the Gaussian.
\be
I = \int_{-\infty}^\infty dx e^{-x^2}
\ee
If the limits are not $\infty$ than you can get the special function known as the \textbf{Error Fucntion}.
Basically a special function is a common or useful function that appears but cannot be evaluated using elementary functions, it is useful wso we give it a name.
%==============================================================================%
%       May want ot add in comments about error functions since he mentioned it
%==============================================================================%
There is a well known trick for evaluating a Gaussian Integral.
You instead consider the square of the integral, and convert to polar coordinates to see a nice simplification.
\be
\begin{split}
	I &= \int_{-\infty}^\infty dx e^{-x^2} \Rightarrow \\
	I^2 &= \left(\int_{-\infty}^\infty dx e^{-x^2}\right) \left(\int_{-\infty}^\infty dy e^{-y^2}\right) \\
	 &= \int_{-\infty}^\infty \int_{-\infty}^\infty dxdy e^{-x^2 - y^2} \\
\end{split}
\ee
We can now switch over to polar coordinates and re-define the integrals
%==============================================================================%
%      Someone add in conversation reviewing polar coordinates
%==============================================================================%
\be
\begin{split}
	 I^2 &= \int_{-\infty}^\infty \int_{-\infty}^\infty dxdy e^{-x^2 - y^2} \\
	 &= \int_0^{2\pi} d\theta \int_0^\infty dr e^{0r^2}r \\
	 &= 2\pi \int_0^\infty dr re^{-r^2} \rightarrow[t=r^2]\\
	 I^2&= 2 \int_0^\infty dt e^{-t} = \pi \\
	 I &= \sqrt{\pi}
\end{split}
\ee

Consider now a related example
\be
I = \int dx e^x \sin(x)
\ee
We know that we can rewrite sin(x) as
\be
\sin(x) = \frac{e^{ix}-e^{-ix}}{2i}
\ee
If we make the substitution we find integrals of the form
\be
\int e^{ax} dx
\ee
Where a is potentially complex and we can work through the math.
%==============================================================================%
%      Someone can try to solve as a gaussian if they want
%==============================================================================%
However, we can also just use integration by parts to evaluate this integral.
%==============================================================================%
%      Someone show more algebra for the IBP
%==============================================================================%
\be
\begin{split}
	I &= \int dx e^x \sin(x) = -e^x \cos(x) + \int dx e^x \cos(x) \rightarrow[\text{IBP Again}]\\
	I &= -e^x \cos(x) + e^x \sin)x) - I = I = \frac{-e^x \cos(x) + e^x \sin(x)}{2}
\end{split}
\ee

\subsection*{Integrals of Rational Functions}
We can now consider anpother class of integrals.
Take P(x) to be a polynomial of degree n: P$_0$ + P$_1$x + $\cdots$ P$_n$x$^n$, and Q(x) to be a polynomial of degree m  Q$_0$ + Q$_1$x + $\cdots$ Q$_n$x$^n$.
\be
\int dx \frac{P(x)}{Q(x)}
\ee
To evaluate we will assume n $<$ m, which is always true because we can simply reduce by dividing by polynomials i.e. if m $>$ n than just divid the polynomials.

It is always possible to find the roots of a polynomial, can be done analytically if degree 4 or less, numerically if higher degree polynomials are involved.
SO it is safe to assume the roots exist, we will assume the roots are not degenerate for convenience.
This means Q can be written as Q(x) = (x-x$_1$) $\cdots$ (x-x$_m$).
\be
\frac{P(x)}{Q(x)} = \frac{P(x)}{(x-x_1)(x-x_2)\cdots(x-x_m)} = \frac{A_1}{x-x_1} + \cdots + \frac{A_m}{x-x_m}
\ee

%==============================================================================%
%      Someone check if it is partial fraction decomposition
%==============================================================================%
Where the coefficients A$_i$ can be found through% (partial fraction decomposition i believe it is called????). %
For example
%==============================================================================%
%      We need to check this vlad made it up on teh spot may be wrong
%==============================================================================%
\be
\frac{1}{(x-a)(x-b)} = \frac{A}{x-a} \frac{B}{x-b} \Rightarrow \frac{A(x-b) + B(x-a)}{(x-a)(x-b)} = 1
\ee
Doing some algebra we find
\be
A = \frac{1}{b-a}, \quad B = \frac{1}{b-a}, \quad \frac{1}{b-a} \left(\frac{1}{x-a} - \frac{1}{x-b}\right)
\ee
%==============================================================================%
%     I had this written kind of randomly not sure were it goes
%==============================================================================%
$\int \frac{A_k}{x-x_k} = A_k \ln(x-x_k)$
%==============================================================================%
%     I had this written kind of randomly not sure were it goes
%==============================================================================%

If we consider a different probelm with degenerate roots we find a similar result.
\be
\frac{1}{(x-a)^2(x-b)} = \frac{A_1x + A_0}{(x-a)^2} + \frac{B}{(x-b)}
\ee
And go through the process to find our new constants. another example.
\be
\frac{1}{(x-a)^2(x-b)^3} = \frac{A_1x + A_0}{(x-a)^2} + \frac{B_2x^2 + B_1x + B_0}{(x-b)^3}
\ee
And again we need to solve a system of equations to find the constants.

Consider a final strategy for evaluating integrals known as \textbf{Parameter Differentation}.
%==============================================================================%
%    Re-Do formatting, just wanted to finish q_q
%==============================================================================%
\be
I_0(a) = \int_0^\infty dx e^{-ax^2} = \frac{1}{2} \sqrt{\frac{\pi}{4}}
\ee
\be
I_1(a) = \int_0^\infty dx e^{-ax^2}x = \frac{1}{2a}
\ee
How do we solve I$_2$(a)?
Consider a derivative
\be
\frac{d}{da} I_0(a) = \int_0^\infty dx (-x^2) e^{-ax^2}
\ee

\be
I_2(a) = \int dx x^2e^{-ax^2} = \frac{1}{4a} \sqrt{\frac{\pi}{a}}
\ee

\be
I_{2n}(a) = \int dx x^{2n}e^{-ax^2} = \frac{1}{4a} \sqrt{\frac{\pi}{a}}
\ee

\be
\frac{d}{da} I_{2n}(a) \Rightarrow _{2n+1} a
\ee
The above is for even powers, and for odd powers we would apply.

\be
\frac{d}{da} I_1(a) \Rightarrow _{2n-1} a
\ee

\end{document}
